{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7309706,"sourceType":"datasetVersion","datasetId":4236219}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\ndf_p_f = pd.read_csv('/kaggle/input/fakenewsnet/full_dataset/politifact_fake.csv')\ndf_p_r = pd.read_csv('/kaggle/input/fakenewsnet/full_dataset/politifact_real.csv')\ndf_g_f = pd.read_csv('/kaggle/input/fakenewsnet/full_dataset/gossipcop_fake.csv')\ndf_g_rs = [pd.read_csv(f'/kaggle/input/fakenewsnet/full_dataset/gossipcop_real_{i}.csv') for i in range(1,5)]\ndf_g_r = pd.concat(df_g_rs,ignore_index = True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-01T23:49:41.358928Z","iopub.execute_input":"2024-01-01T23:49:41.359209Z","iopub.status.idle":"2024-01-01T23:49:46.469438Z","shell.execute_reply.started":"2024-01-01T23:49:41.359184Z","shell.execute_reply":"2024-01-01T23:49:46.468585Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndef parse_url(url):\n    if url != url:\n        return np.nan\n    id1 = url.find('//')\n    if id1>=0:\n        url = url[id1+2:]\n    id2 = url.find('www.')\n    if id2>=0:\n        url = url[id2+4:]\n    id3 = url.find('/')\n    url = url[:id3]\n    return url","metadata":{"execution":{"iopub.status.busy":"2024-01-01T23:49:46.470948Z","iopub.execute_input":"2024-01-01T23:49:46.471249Z","iopub.status.idle":"2024-01-01T23:49:46.477629Z","shell.execute_reply.started":"2024-01-01T23:49:46.471223Z","shell.execute_reply":"2024-01-01T23:49:46.476261Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df_p_f['parsed_url'] = df_p_f['news_url'].apply(lambda x: parse_url(x))\ndf_p_f['label'] = 0\ndf_p_r['parsed_url'] = df_p_r['news_url'].apply(lambda x: parse_url(x))\ndf_p_r['label'] = 1\ndf_g_f['parsed_url'] = df_g_f['news_url'].apply(lambda x: parse_url(x))\ndf_g_f['label'] = 0\ndf_g_r['parsed_url'] = df_g_r['news_url'].apply(lambda x: parse_url(x))\ndf_g_r['label'] = 1","metadata":{"execution":{"iopub.status.busy":"2024-01-01T23:49:46.478985Z","iopub.execute_input":"2024-01-01T23:49:46.479245Z","iopub.status.idle":"2024-01-01T23:49:46.533764Z","shell.execute_reply.started":"2024-01-01T23:49:46.479222Z","shell.execute_reply":"2024-01-01T23:49:46.532806Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"everything_df = pd.concat([df_g_f,df_g_r,df_p_f,df_p_r],ignore_index = True)\neverything_df.dropna(subset = ['parsed_url'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T23:49:46.535885Z","iopub.execute_input":"2024-01-01T23:49:46.536195Z","iopub.status.idle":"2024-01-01T23:49:46.561455Z","shell.execute_reply.started":"2024-01-01T23:49:46.536168Z","shell.execute_reply":"2024-01-01T23:49:46.560568Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"set_true = list(set(everything_df[everything_df['label']==1]['parsed_url']))\nset_false = list(set(everything_df[everything_df['label']==0]['parsed_url']))","metadata":{"execution":{"iopub.status.busy":"2024-01-01T23:49:46.562636Z","iopub.execute_input":"2024-01-01T23:49:46.562907Z","iopub.status.idle":"2024-01-01T23:49:46.578468Z","shell.execute_reply.started":"2024-01-01T23:49:46.562884Z","shell.execute_reply":"2024-01-01T23:49:46.577741Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"to_classify = everything_df[(everything_df['parsed_url'].isin(set_true)) & (everything_df['parsed_url'].isin(set_false))]","metadata":{"execution":{"iopub.status.busy":"2024-01-01T23:49:46.579472Z","iopub.execute_input":"2024-01-01T23:49:46.579777Z","iopub.status.idle":"2024-01-01T23:49:46.594734Z","shell.execute_reply.started":"2024-01-01T23:49:46.579752Z","shell.execute_reply":"2024-01-01T23:49:46.593810Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"! pip install happytransformer evaluate\n\nfrom happytransformer import HappyTextClassification\nhappy_tc = HappyTextClassification(\"BERT\", \"bert-base-uncased\", num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T23:49:46.596164Z","iopub.execute_input":"2024-01-01T23:49:46.596645Z","iopub.status.idle":"2024-01-01T23:50:17.538065Z","shell.execute_reply.started":"2024-01-01T23:49:46.596612Z","shell.execute_reply":"2024-01-01T23:50:17.536874Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting happytransformer\n  Obtaining dependency information for happytransformer from https://files.pythonhosted.org/packages/f6/ed/8abe77d280294a454534003242431439b102cf17e1089fde6020f90ab621/happytransformer-3.0.0-py3-none-any.whl.metadata\n  Downloading happytransformer-3.0.0-py3-none-any.whl.metadata (4.4 kB)\nCollecting evaluate\n  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: torch>=1.0 in /opt/conda/lib/python3.10/site-packages (from happytransformer) (2.0.0)\nRequirement already satisfied: tqdm>=4.43 in /opt/conda/lib/python3.10/site-packages (from happytransformer) (4.66.1)\nRequirement already satisfied: transformers<5.0.0,>=4.30.1 in /opt/conda/lib/python3.10/site-packages (from happytransformer) (4.36.0)\nCollecting datasets<3.0.0,>=2.13.1 (from happytransformer)\n  Obtaining dependency information for datasets<3.0.0,>=2.13.1 from https://files.pythonhosted.org/packages/ec/93/454ada0d1b289a0f4a86ac88dbdeab54921becabac45da3da787d136628f/datasets-2.16.1-py3-none-any.whl.metadata\n  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from happytransformer) (0.1.99)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from happytransformer) (3.20.3)\nRequirement already satisfied: accelerate<1.0.0,>=0.20.1 in /opt/conda/lib/python3.10/site-packages (from happytransformer) (0.25.0)\nRequirement already satisfied: tokenizers<1.0.0,>=0.13.3 in /opt/conda/lib/python3.10/site-packages (from happytransformer) (0.15.0)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (from happytransformer) (0.16.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.12.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (6.0.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (0.4.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (3.12.2)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (11.0.0)\nCollecting pyarrow-hotfix (from datasets<3.0.0,>=2.13.1->happytransformer)\n  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nCollecting fsspec[http]>=2021.05.0 (from evaluate)\n  Obtaining dependency information for fsspec[http]>=2021.05.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (3.8.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.30.1->happytransformer) (2023.8.8)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->happytransformer) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->happytransformer) (3.1.32)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->happytransformer) (1.39.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb->happytransformer) (0.4.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb->happytransformer) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->happytransformer) (68.1.2)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb->happytransformer) (1.4.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (1.3.1)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->happytransformer) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->happytransformer) (4.0.10)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0->happytransformer) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0->happytransformer) (1.3.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->happytransformer) (5.0.0)\nDownloading happytransformer-3.0.0-py3-none-any.whl (24 kB)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nInstalling collected packages: pyarrow-hotfix, fsspec, datasets, happytransformer, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.12.2\n    Uninstalling fsspec-2023.12.2:\n      Successfully uninstalled fsspec-2023.12.2\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.16.1 evaluate-0.4.1 fsspec-2023.10.0 happytransformer-3.0.0 pyarrow-hotfix-0.6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"811fdf707e8b44d2bfec6f7fec6a5d0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2467a3774b3449dda8510368cedced09"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"625be6464c1a40ad8cc0827b23077e4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ce0526e1b1c461aa90446dd80b37104"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"806950bc7161471591781e4bc479d8a7"}},"metadata":{}}]},{"cell_type":"code","source":"import re\nimport nltk\nimport spacy\nimport string\nfrom nltk.corpus import stopwords\nnltk.download('wordnet')\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\nfrom nltk.corpus import wordnet\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2024-01-01T23:50:17.539415Z","iopub.execute_input":"2024-01-01T23:50:17.540310Z","iopub.status.idle":"2024-01-01T23:50:18.913128Z","shell.execute_reply.started":"2024-01-01T23:50:17.540278Z","shell.execute_reply":"2024-01-01T23:50:18.912143Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\nPUNCT_TO_REMOVE = string.punctuation\nSTOPWORDS = set(stopwords.words('english'))\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n\ndef preprocess (text):\n    def lower(text):\n        return text.lower()\n    def remove_punctuation(text):\n        return text.translate(text.maketrans('', '', PUNCT_TO_REMOVE))\n    def remove_stopwords(text):\n        return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n    def stem_words(text):\n        return \" \".join([stemmer.stem(word) for word in text.split()])\n    def lemmatize_words(text):\n        pos_tagged_text = nltk.pos_tag(text.split())\n        return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n    return lemmatize_words(stem_words(remove_stopwords(remove_punctuation(lower(text)))))","metadata":{"execution":{"iopub.status.busy":"2024-01-01T23:50:18.914664Z","iopub.execute_input":"2024-01-01T23:50:18.914967Z","iopub.status.idle":"2024-01-01T23:50:21.169058Z","shell.execute_reply.started":"2024-01-01T23:50:18.914936Z","shell.execute_reply":"2024-01-01T23:50:21.168271Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"to_classify_2 = to_classify[~to_classify['baseline'].isna()]\n#to_classify_2['baseline_process'] = to_classify_2['baseline'].apply(lambda x: preprocess(x))","metadata":{"execution":{"iopub.status.busy":"2024-01-02T00:15:56.763621Z","iopub.execute_input":"2024-01-02T00:15:56.764001Z","iopub.status.idle":"2024-01-02T00:15:56.777487Z","shell.execute_reply.started":"2024-01-02T00:15:56.763972Z","shell.execute_reply":"2024-01-02T00:15:56.776548Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"to_classify_2[to_classify_2['baseline'].isna()]","metadata":{"execution":{"iopub.status.busy":"2024-01-02T00:16:19.567243Z","iopub.execute_input":"2024-01-02T00:16:19.567644Z","iopub.status.idle":"2024-01-02T00:16:19.580084Z","shell.execute_reply.started":"2024-01-02T00:16:19.567614Z","shell.execute_reply":"2024-01-02T00:16:19.579190Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [id, news_url, title, tweet_ids, text, baseline, parsed_url, label]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>news_url</th>\n      <th>title</th>\n      <th>tweet_ids</th>\n      <th>text</th>\n      <th>baseline</th>\n      <th>parsed_url</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_data = to_classify_2.sample(n=int(0.8*len(to_classify)))\ntest_data = to_classify_2[~to_classify_2.id.isin(train_data.id)]","metadata":{"execution":{"iopub.status.busy":"2024-01-02T00:18:31.613505Z","iopub.execute_input":"2024-01-02T00:18:31.614205Z","iopub.status.idle":"2024-01-02T00:18:31.630975Z","shell.execute_reply.started":"2024-01-02T00:18:31.614171Z","shell.execute_reply":"2024-01-02T00:18:31.630179Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"train_data.reset_index(inplace=True)\ntest_data.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T00:18:34.104138Z","iopub.execute_input":"2024-01-02T00:18:34.104504Z","iopub.status.idle":"2024-01-02T00:18:34.110082Z","shell.execute_reply.started":"2024-01-02T00:18:34.104475Z","shell.execute_reply":"2024-01-02T00:18:34.109142Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import csv\ntrain_cases = train_data['baseline']\ntrain_labels = train_data['label']\n\nwith open(\"/kaggle/working/train.csv\", \"w\") as f:\n   writer = csv.writer(f)\n   writer.writerow(['text', 'label'])\n   writer.writerows(zip(train_cases, train_labels))","metadata":{"execution":{"iopub.status.busy":"2024-01-02T00:18:36.282628Z","iopub.execute_input":"2024-01-02T00:18:36.283042Z","iopub.status.idle":"2024-01-02T00:18:39.011457Z","shell.execute_reply.started":"2024-01-02T00:18:36.283012Z","shell.execute_reply":"2024-01-02T00:18:39.010391Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"from happytransformer import TCTrainArgs\nargs = TCTrainArgs(batch_size=8)\nhappy_tc.train(\"/kaggle/working/train.csv\", args=args)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T00:18:58.140856Z","iopub.execute_input":"2024-01-02T00:18:58.141690Z","iopub.status.idle":"2024-01-02T00:35:07.933287Z","shell.execute_reply.started":"2024-01-02T00:18:58.141656Z","shell.execute_reply":"2024-01-02T00:35:07.932478Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26a133d069684655b837a5b16d40dd4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing data:   0%|          | 0/12384 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8b03c9a7dea465890b08b761764e916"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing data:   0%|          | 0/1376 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2c795dacda64edc82324000a257b74a"}},"metadata":{}},{"name":"stderr","text":"You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1548' max='1548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1548/1548 15:33, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.758700</td>\n      <td>0.655459</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>0.529000</td>\n      <td>0.456434</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.514300</td>\n      <td>0.497632</td>\n    </tr>\n    <tr>\n      <td>465</td>\n      <td>0.487700</td>\n      <td>0.504992</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.517500</td>\n      <td>0.467385</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>0.463700</td>\n      <td>0.459931</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.444500</td>\n      <td>0.464550</td>\n    </tr>\n    <tr>\n      <td>1085</td>\n      <td>0.418100</td>\n      <td>0.418509</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.416500</td>\n      <td>0.427545</td>\n    </tr>\n    <tr>\n      <td>1395</td>\n      <td>0.399600</td>\n      <td>0.439754</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import tqdm\npredictions = []\n\nfor case in tqdm(test_data['baseline']):\n  output = happy_tc.classify_text(case[:512]).label\n  if output == \"LABEL_0\":\n    predictions.append(0)\n  else:\n    predictions.append(1)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T00:40:47.123063Z","iopub.execute_input":"2024-01-02T00:40:47.123481Z","iopub.status.idle":"2024-01-02T00:41:02.936030Z","shell.execute_reply.started":"2024-01-02T00:40:47.123447Z","shell.execute_reply":"2024-01-02T00:41:02.935138Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"  1%|          | 9/1377 [00:00<00:16, 83.76it/s]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n100%|██████████| 1377/1377 [00:15<00:00, 87.14it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\naccuracy_metric = evaluate.load(\"accuracy\")\nprecision_metric = evaluate.load(\"precision\")\nrecall_metric = evaluate.load(\"recall\")\ntest_labels = test_data['label']\naccuracy_output = accuracy_metric.compute(references=test_labels, predictions=predictions)\nprint(accuracy_output)\nprecision__output = precision_metric.compute(references=test_labels, predictions=predictions)\nprint(precision__output)\nrecall_output = recall_metric.compute(references=test_labels, predictions=predictions)\nprint(recall_output)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T00:41:20.898830Z","iopub.execute_input":"2024-01-02T00:41:20.899159Z","iopub.status.idle":"2024-01-02T00:41:23.370201Z","shell.execute_reply.started":"2024-01-02T00:41:20.899134Z","shell.execute_reply":"2024-01-02T00:41:23.369334Z"},"trusted":true},"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edb476ed1d114042a0f2021d3fc636bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2ea5cdb5b694a06aa49004f05d2b8f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a75273576ce24dbc8ef59f19d8f6fe4e"}},"metadata":{}},{"name":"stdout","text":"{'accuracy': 0.8358750907770516}\n{'precision': 0.8395061728395061}\n{'recall': 0.9705042816365367}\n","output_type":"stream"}]},{"cell_type":"code","source":"2/(1/0.8395061728395061+1/0.9705042816365367)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T00:42:38.153314Z","iopub.execute_input":"2024-01-02T00:42:38.153705Z","iopub.status.idle":"2024-01-02T00:42:38.160678Z","shell.execute_reply.started":"2024-01-02T00:42:38.153673Z","shell.execute_reply":"2024-01-02T00:42:38.159704Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"0.9002647837599294"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}